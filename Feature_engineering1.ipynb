{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e65038fa-d64d-45a9-8c49-96ec878a73a8",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its \n",
    "application.\n",
    "Ans:-Min-Max scaling, also known as min-max normalization or feature scaling, is a data preprocessing technique used to scale numeric features in a way that transforms them to a specific range, typically between 0 and 1. The purpose of Min-Max scaling is to ensure that all features contribute equally to the analysis and to prevent features with larger scales from dominating the learning process in machine learning algorithms.\n",
    "Example:\n",
    "\r\n",
    "Suppose you have a dataset with a feature, \"House Area,\" representing the size of houses in square feet. The \"House Area\" feature has values ranging from 800 to 2000 square feet. You want to apply Min-Max scaling to bring these values into a standardized range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cae424-eecb-4b8f-b8ae-cb5391f3ac21",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? \n",
    "Provide an example to illustrate its application.\n",
    "Ans:-The Unit Vector technique, also known as vector normalization or feature scaling, is a method used to scale numeric features by dividing each data point by the Euclidean norm (magnitude) of the entire feature vector. This process transforms the features into a unit vector, meaning that the magnitude of the vector becomes 1. Unit Vector scaling is particularly useful when the direction of the feature vector is more important than its magnitude.\n",
    "Example:\n",
    "\r\n",
    "Suppose you have a dataset with two features, \"House Area\" and \"Number of Bedrooms.\" The feature vector for each data point is\r\n",
    "2\r\n",
    "]\r\n",
    "[800,2] for \"House Area\" and \"Number of Bedrooms,\" respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7406dc-e561-48e2-be28-cc335436e53e",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an \n",
    "example to illustrate its application.\n",
    "Ans:-Principal Component Analysis (PCA) is a dimensionality reduction technique commonly used in machine learning and statistics. The primary goal of PCA is to transform high-dimensional data into a new coordinate system (principal components) where the data's variability is maximized along the axes. This allows for the reduction of the number of features (dimensions) while retaining as much of the original data's information as possible.\n",
    "\r\n",
    "The steps involved in PCA are as follow:\r\n",
    "\r\n",
    "Standardize the Dta:\r\n",
    "\r\n",
    "Standardize the features to have zero mean and unit variance. This step ensures that all features contribute equally to the analysis.\r\n",
    "Calculate the Covariance atrix:\r\n",
    "\r\n",
    "Compute the covariance matrix for the standardized data. The covariance matrix represents the relationships between different features.\r\n",
    "Calculate Eigenvectors and Eienvalues:\r\n",
    "\r\n",
    "Find the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors represent the directions of maximum variance, while eigenvalues indicate the magnitude of variance in each direction.\r\n",
    "Sort Eigenvectors byEigenvalues:\r\n",
    "\r\n",
    "Sort the eigenvectors based on their corresponding eigenvalues in descending order. The eigenvector with the highest eigenvalue represents the principal component with the highest variance.\r\n",
    "Select Princpal Components:\r",
    "oose the top \r\n",
    "ï¿½\r\n",
    "k eigenvectors to form a new matrix called the projection matrix. This matrix is used to transform the original data into a lower-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5964a358-adab-44d0-9551-dcc9f3b544f4",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature \n",
    "Extraction? Provide an example to illustrate this concept.\n",
    "Ans:-PCA (Principal Component Analysis) and feature extraction are closely related concepts. PCA is a specific technique for feature extraction, which involves transforming the original features of a dataset into a new set of features (principal components) that capture the most significant information in the data. Feature extraction, in general, refers to methods that transform raw data into a reduced and more informative representation.\n",
    "\r\n",
    "The relationship between PCA and feature extraction can be summarized as follow:\r\n",
    "\r\n",
    "Dimensionality Reducton:\r\n",
    "\r\n",
    "Both PCA and feature extraction aim to reduce the dimensionality of the data. By selecting a subset of the most relevant features or by creating new features (principal components), the goal is to represent the data in a lower-dimensional space while preserving as much of the original information as possible.\r\n",
    "Information Retntion:\r\n",
    "\r\n",
    "PCA is designed to maximize the variance captured by the principal components. In the context of feature extraction, the emphasis is on retaining the most informative aspects of the data, discarding less critical information to simplify the representation.\r\n",
    "Orthogonal Transormation:\r\n",
    "\r\n",
    "PCA performs an orthogonal transformation to convert the original correlated features into a set of linearly uncorrelated principal components. This transformation facilitates the removal of redundant information and helps identify patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee49ae6-3ef9-4037-aecb-60e14cb427c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample data\n",
    "data = [[160, 55, 25],\n",
    "        [165, 60, 30],\n",
    "        [170, 65, 35],\n",
    "        [175, 70, 40],\n",
    "        [180, 75, 45]]\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)  # Choose the number of components\n",
    "principal_components = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Display the transformed data\n",
    "print(\"Transformed Data (Principal Components):\")\n",
    "print(principal_components)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12248c1e-43bc-4bdf-97e4-08f1d6eb6ae3",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset \n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to \n",
    "preprocess the data.\n",
    "Ans:-In the context of building a recommendation system for a food delivery service, Min-Max scaling can be used to preprocess the data, ensuring that the features are on a similar scale. This is particularly important when features have different units or ranges, as it helps prevent certain features from dominating the recommendation process based on their numerical scale. Here's how you can use Min-Max scaling for preprocessing:\n",
    "\r\n",
    "Identify Feature:\r\n",
    "\r\n",
    "Identify the relevant features in your dataset that you want to include in the recommendation system. These could be features such as price, rating, delivery time, and any other attributes that may influence recommendations.\r\n",
    "Understand Feature Rages:\r\n",
    "\r\n",
    "Examine the ranges of each feature. For example, prices may range from low to high values, ratings may range from 1 to 5, and delivery time may be measured in minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197286a9-d7d0-48f5-ad77-145f4050a788",
   "metadata": {},
   "outputs": [],
   "source": [
    "Original Dataset:\n",
    "Price | Rating | Delivery Time\n",
    "------|--------|---------------\n",
    "10    | 4.5    | 30\n",
    "20    | 3.0    | 45\n",
    "15    | 4.8    | 25\n",
    "\n",
    "Scaled Dataset:\n",
    "Price | Rating | Delivery Time\n",
    "------|--------|---------------\n",
    "0.25  | 0.75   | 0.5\n",
    "0.75  | 0.0    | 1.0\n",
    "0.5   | 1.0    | 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd560aa-ab32-47af-b412-6a33f1566e17",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many \n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the \n",
    "dimensionality of the dataset.\n",
    "Ans:-When dealing with a large number of features in a dataset for predicting stock prices, Principal Component Analysis (PCA) can be a valuable tool for reducing dimensionality while preserving the most important information. Here's a step-by-step explanation of how you would use PCA for dimensionality reduction in the context of predicting stock prices:\n",
    "\r\n",
    "Understand the Datase:\r\n",
    "\r\n",
    "Start by understanding the structure of your dataset. Identify the features related to company financial data, market trends, and any other relevant information that may impact stock prices.\r\n",
    "Standardize the ata:\r\n",
    "\r\n",
    "Standardize the dataset by ensuring that all features have zero mean and unit variance. This step is crucial for PCA since it is sensitive to the scale of the features.\r\n",
    "Aply PCA:\r\n",
    "\r\n",
    "Use PCA to calculate the principal components of the standardized dataset. PCA will transform the original features into a set of linearly uncorrelated principal components, ordered by the amount of variance they capture.\r\n",
    "Determine the Number of omponents:\r\n",
    "\r\n",
    "Evaluate the explained variance ratio for each principal component. The explained variance ratio indicates the proportion of the total variance in the data explained by each component. Decide on the number of components to retain based on how much variance you want to preserve. You can plot the cumulative explained variance to help make this decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d8aec3-4696-4f20-b99b-6eb54807f85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume X is your standardized dataset\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "\n",
    "# Plot cumulative explained variance\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), \n",
    "         np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dc0730-5275-457c-9c7b-483796979682",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the \n",
    "values to a range of -1 to 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
